---
layout: post
title: Kullbackâ€“Leibler divergence
subtitle: Information loss in distribution approximation 
#cover-img: /assets/img/path.jpg
#thumbnail-img: /assets/img/measure-1509707_640.jpg
#share-img: /assets/img/path.jpg
tags: [probability theory, information theory]
---

> To measure the difference between two probability distributions over the same
variable x, a measure, called the Kullback-Leibler divergence, or simply, the KL
divergence, has been popularly used in the data mining literature. The concept
was originated in probability theory and information theory.
The KL divergence, which is closely related to relative entropy, information divergence, and information for discrimination, is a non-symmetric measure of the difference between two probability distributions p(x) and q(x).
Specifically, the Kullback-Leibler (KL) divergence of q(x) from p(x), denoted
DKL(p(x), q(x)), is a measure of the information lost when q(x) is used to approximate p(x).

[Kullback-Leibler Divergence](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf)

 By: Alex Javidi
