---
layout: post
title: Meta-analysis
subtitle: No single study is a pure reflection of the underlying truth
#cover-img: /assets/img/path.jpg
#thumbnail-img: /assets/img/measure-1509707_640.jpg
#share-img: /assets/img/path.jpg
tags: [probability theory, combinatorial problems]
---


-> Variability in findings across studies can be uniquely troubling when replications are conducted. 
Because replications are often used to assess the trustworthiness of an effect (e.g., Open Science Collaboration [11]), 
if an effect cannot be replicated, the finding, and sometimes the associated researchers, may be viewed as questionable.
As a result, a replication being interpreted as a success or a failure can have severe consequences for the area of study and perceptions of the researchers’
integrity [12–13]. However, a problem with interpreting replications is that single studies, even if competently executed and properly analyzed, 
are inherently imperfect pieces of information

```
One approach for evaluating replications has been to compare the p-value of the original study to that of the replication study. 
An application of this approach involves checking whether the direction of an effect and its classification as statistically significant or
statistically non-significant are consistent across the replication and original study. If there is consistency, the replication is deemed successful;
however, if there is inconsistency the replication is a failure. 
Aside from the well-expressed issues of relying solely on p-values to interpret results [16, 21–24], Cumming [25] illustrates that this is a flawed approach
because p-values fluctuate so considerably across replication attempts, due to sampling error, making it a poor criterion to evaluate replications.
```
[Prediction Interval: What to Expect When You’re Expecting … A Replication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028066/)
